{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13575b15",
   "metadata": {},
   "source": [
    "# Attempt at understanding the influence of hyperparameters along the full sequence of the code execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125bbfc4",
   "metadata": {},
   "source": [
    "## Imports & logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188109f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "from tables_io import io\n",
    "from rail.estimation.estimator import Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad712b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from rail.estimation.estimator import Estimator as BaseEstimation\n",
    "from rail.estimation.utils import check_and_print_params\n",
    "import qp\n",
    "import errno\n",
    "import coloredlogs\n",
    "import logging\n",
    "from pkg_resources import resource_filename\n",
    "\n",
    "# Delight initialisation\n",
    "# Filters and SED\n",
    "from rail.estimation.algos.include_delightPZ.processFilters import processFilters\n",
    "from rail.estimation.algos.include_delightPZ.processSEDs import processSEDs  # build a redshift -flux grid model\n",
    "\n",
    "# interface with Delight through files\n",
    "from rail.estimation.algos.include_delightPZ.makeConfigParam import *  # build the parameter file required by Delight\n",
    "\n",
    "# Delight format\n",
    "from rail.estimation.algos.include_delightPZ.convertDESCcat  import *   # convert DESC input file into Delight format\n",
    "\n",
    "# mock data simulation\n",
    "from rail.estimation.algos.include_delightPZ.simulateWithSEDs import simulateWithSEDs # simulate its own SED in tutorial mode\n",
    "\n",
    "# Delight algorithms\n",
    "#from rail.estimation.algos.include_delightPZ.templateFitting import templateFitting\n",
    "#from rail.estimation.algos.include_delightPZ.delightLearn import delightLearn\n",
    "#from rail.estimation.algos.include_delightPZ.delightApply import delightApply\n",
    "\n",
    "# other\n",
    "#from rail.estimation.algos.include_delightPZ.calibrateTemplateMixturePriors import *\n",
    "from rail.estimation.algos.include_delightPZ.getDelightRedshiftEstimation import *\n",
    "\n",
    "# Create a logger object.\n",
    "logger = logging.getLogger(__name__)\n",
    "coloredlogs.install(level='DEBUG', logger=logger,fmt='%(asctime)s,%(msecs)03d %(programname)s %(name)s[%(process)d] %(levelname)s %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d926b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail.estimation.algos.include_delightPZ.delight_io import *\n",
    "from delight.utils import *\n",
    "from delight.photoz_gp import PhotozGP\n",
    "from delight.photoz_kernels import Photoz_mean_function, Photoz_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af05c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from rail.estimation.algos.include_delightPZ.libPriorPZ import *\n",
    "FLAG_NEW_PRIOR = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb6539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delight.utils_cy import approx_flux_likelihood_cy\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3a4816",
   "metadata": {},
   "source": [
    "## Init (~RAIL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca76ebc3",
   "metadata": {},
   "source": [
    "### ~Rail main.py call - try to remove the .yaml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc1d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This is where 'base.yaml' actually belongs, but how to make it so\n",
    "def main(argv):\n",
    "    if len(argv) == 2:\n",
    "        # this is in case hiding the base yaml is wanted\n",
    "        input_yaml = argv[1]\n",
    "        base_config = 'base.yaml'\n",
    "    elif len(argv) == 3:\n",
    "        input_yaml = argv[1]\n",
    "        base_config = argv[2]\n",
    "    else:\n",
    "        print(len(argv))\n",
    "        print(\"Usage: main <config yaml file> [base config yaml]\")\n",
    "        sys.exit()\n",
    "\n",
    "    with open(input_yaml, 'r') as f:\n",
    "        run_dict = yaml.safe_load(f)\n",
    "\n",
    "    name = run_dict['run_params']['class_name']\n",
    "\n",
    "    try:\n",
    "        Estimator._find_subclass(name)\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Class name {name} for PZ code is not defined\")\n",
    "\n",
    "    code = Estimator._find_subclass(name)\n",
    "    print(f\"code name: {name}\")\n",
    "\n",
    "    pz = code(base_config, run_dict)\n",
    "\n",
    "    pz.inform_dict = run_dict['run_params']['inform_options']\n",
    "    if pz.inform_dict['load_model']:\n",
    "        # note: specific options set in subclasss func def\n",
    "        pz.load_pretrained_model()\n",
    "    else:\n",
    "        trainfile = pz.trainfile\n",
    "        train_fmt = trainfile.split(\".\")[-1]\n",
    "        training_data = io.read(trainfile,\n",
    "                                None,\n",
    "                                train_fmt,\n",
    "                                )[pz.groupname]\n",
    "        pz.inform(training_data)\n",
    "\n",
    "    if 'run_name' in run_dict['run_params']:\n",
    "        outfile = run_dict['run_params']['run_name'] + '.hdf5'\n",
    "        tmpfile = \"temp_\" + outfile\n",
    "    else:\n",
    "        outfile = 'output.hdf5'\n",
    "\n",
    "    if pz.output_format == 'qp':\n",
    "        tmploc = os.path.join(pz.outpath, name, tmpfile)\n",
    "        outfile = run_dict['run_params']['run_name'] + \"_qp.hdf5\"\n",
    "    saveloc = os.path.join(pz.outpath, name, outfile)\n",
    "\n",
    "    for chunk, (start, end, data) in enumerate(io.iterHdf5ToDict(pz.testfile,\n",
    "                                                                 pz._chunk_size,\n",
    "                                                                 'photometry')):\n",
    "        pz_data_chunk = pz.estimate(data)\n",
    "        if chunk == 0:\n",
    "            if pz.output_format == 'qp':\n",
    "                group, outf = pz_data_chunk.initializeHdf5Write(saveloc, pz.num_rows)\n",
    "            else:\n",
    "                _, outf = io.initializeHdf5Write(saveloc, None, zmode=((pz.num_rows,), 'f4'),\n",
    "                                                 pz_pdf=((pz.num_rows, pz.nzbins), 'f4'))\n",
    "        if pz.output_format == 'qp':\n",
    "            pz_data_chunk.writeHdf5Chunk(group, start, end)\n",
    "        else:\n",
    "            io.writeDictToHdf5Chunk(outf, pz_data_chunk, start, end)\n",
    "        print(\"writing \" + name + f\"[{start}:{end}]\")\n",
    "\n",
    "    num_chunks = end // pz._chunk_size\n",
    "    if end % pz._chunk_size > 0:\n",
    "        num_chunks += 1\n",
    "\n",
    "    if pz.output_format == 'qp':\n",
    "        pz_data_chunk.finalizeHdf5Write(outf)\n",
    "    else:\n",
    "        io.finalizeHdf5Write(outf, zgrid=pz.zgrid)\n",
    "    print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da735067",
   "metadata": {},
   "source": [
    "### DelightPZ class creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d8cfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def_param = dict(run_params = dict(dlght_redshiftMin=0.01,\n",
    "                                   dlght_redshiftMax=3.01,\n",
    "                                   dlght_redshiftNumBinsGPpred = 301,\n",
    "                                   nzbins = 301,\n",
    "                                   dlght_redshiftBinSize = 0.01,\n",
    "                                   dlght_redshiftDisBinSize = 0.2,\n",
    "                                   bands_names = \"DC2LSST_u DC2LSST_g DC2LSST_r DC2LSST_i DC2LSST_z DC2LSST_y\",\n",
    "                                   bands_path = \"./rail/estimation/data/FILTER\",\n",
    "                                   bands_fmt = \"res\",\n",
    "                                   bands_numcoefs = 15,\n",
    "                                   bands_verbose = True,\n",
    "                                   bands_makeplots = False,\n",
    "                                   bands_debug = True,\n",
    "                                   tempdir = \"./examples/estimation/tmp\",\n",
    "                                   tempdatadir = \"./examples/estimation/tmp/delight_data\",\n",
    "                                   sed_path = \"./rail/estimation/data/SED\",\n",
    "                                   sed_name_list = \"El_B2004a Sbc_B2004a Scd_B2004a SB3_B2004a SB2_B2004a Im_B2004a ssp_25Myr_z008 ssp_5Myr_z008\",\n",
    "                                   sed_fmt = \"sed\",\n",
    "                                   prior_t_list = \"0.27 0.26 0.25 0.069 0.021 0.11 0.0061 0.0079\",\n",
    "                                   prior_zt_list = \"0.23 0.39 0.33 0.31 1.1 0.34 1.2 0.14\",\n",
    "                                   lambda_ref = 4500.,\n",
    "                                   train_refbandorder = \"DC2LSST_u DC2LSST_u_var DC2LSST_g DC2LSST_g_var DC2LSST_r DC2LSST_r_var DC2LSST_i DC2LSST_i_var DC2LSST_z DC2LSST_z_var DC2LSST_y DC2LSST_y_var redshift\",\n",
    "                                   train_refband = \"DC2LSST_i\",\n",
    "                                   train_fracfluxerr = 1.e-4,\n",
    "                                   train_xvalidate = False,\n",
    "                                   train_xvalbandorder = \"_ _ _ _ DC2LSST_r DC2LSST_r_var _ _ _ _ _ _\",\n",
    "                                   target_refbandorder = \"DC2LSST_u DC2LSST_u_var DC2LSST_g DC2LSST_g_var DC2LSST_r DC2LSST_r_var DC2LSST_i DC2LSST_i_var DC2LSST_z DC2LSST_z_var DC2LSST_y DC2LSST_y_var redshift\",\n",
    "                                   target_refband = \"DC2LSST_r\",\n",
    "                                   target_fracfluxerr = 1.e-4,\n",
    "                                   delightparamfile = \"parametersTest.cfg\",\n",
    "                                   dlght_tutorialmode = False,\n",
    "                                   flag_filter_training = True,\n",
    "                                   snr_cut_training = 5,\n",
    "                                   flag_filter_validation = True,\n",
    "                                   snr_cut_validation = 3,\n",
    "                                   dlght_calibrateTemplateMixturePrior = False,\n",
    "                                   dlght_inputdata = \"./examples/estimation/tmp/delight_indata\",\n",
    "                                   zPriorSigma = 0.2,\n",
    "                                   ellPriorSigma = 0.5,\n",
    "                                   fluxLuminosityNorm = 1.0,\n",
    "                                   alpha_C = 1.0e3,\n",
    "                                   V_C = 0.1,\n",
    "                                   alpha_L = 1.0e2,\n",
    "                                   V_L = 0.1,\n",
    "                                   lineWidthSigma = 20,\n",
    "                                   inform_options = dict(save_train = False,\n",
    "                                                         load_model = False,\n",
    "                                                         modelfile = 'model.out'),\n",
    "                                   \n",
    "))\n",
    "\n",
    "\n",
    "desc_dict = dict(dlght_redshiftMin = \"min redshift\",\n",
    "                 dlght_redshiftMax = \"max redshift\",\n",
    "                 dlght_redshiftNumBinsGPpred = \"num bins\",\n",
    "                 dlght_redshiftDisBinSize = \"???\",\n",
    "                 dlght_redshiftBinSize = \"bad, shouldn't be here\",\n",
    "                 nzbins = \"num bins\",\n",
    "                 bands_names = \"string with list of Filter names\",\n",
    "                 bands_path = \"string specifying path to filter directory\",\n",
    "                 bands_fmt = \"string giving the file extension of the filters, not including the '.'\",\n",
    "                 bands_numcoefs = \"integer specifying number of coefs in approximation of filter\",\n",
    "                 bands_verbose = \"boolean filter verbosity\",\n",
    "                 bands_makeplots = \"boolean for whether to make plot showing approximate filters\",\n",
    "                 bands_debug = \"boolean debug flag for filters\",\n",
    "                 tempdir = \"temp dir\",\n",
    "                 tempdatadir = \"temp data dir\",\n",
    "                 sed_path = \"path to seds\",\n",
    "                 sed_name_list = \"String with list of all SED names, with no file extension\",\n",
    "                 sed_fmt = \"file extension of SED files (withough the '.', e.g dat or sed\",\n",
    "                 prior_t_list = \"String of numbers specifying prior type fracs MUST BE SAME LENGTH AS NUMBER OF SEDS\",\n",
    "                 prior_zt_list = \"string of numbers for redshift prior, MUST BE SAME LENGTH AS NUMBER OF SEDS\",\n",
    "                 lambda_ref = \"reference wavelength\",\n",
    "                 train_refbandorder = \"order of bands in training?\",\n",
    "                 train_refband = \"string name of ref band\",\n",
    "                 train_fracfluxerr = \"float: frac err to add to flux?\",\n",
    "                 train_xvalidate = \"bool: cross validate flag\",\n",
    "                 train_xvalbandorder = \"Str: cols to use in cross validation?\",\n",
    "                 target_refbandorder = \"Str: order of reference bands for target data?\",\n",
    "                 target_refband = \"Str: the reference band for the taret data?\",\n",
    "                 target_fracfluxerr = \"float: extra fractional error to add to target fluxes?\",\n",
    "                 delightparamfile = \"param file\",\n",
    "                 dlght_tutorialmode = \"bool: run in tutorial mode\",\n",
    "                 flag_filter_training = \"bool: ?\",\n",
    "                 snr_cut_training = \"SNR training cut\",\n",
    "                 flag_filter_validation = \"bool: ?\",\n",
    "                 snr_cut_validation = \"SNR val cut\",\n",
    "                 dlght_calibrateTemplateMixturePrior = \"bool, ?\",\n",
    "                 dlght_inputdata = \"data dir\",\n",
    "                 zPriorSigma = \"prior thing\",\n",
    "                 ellPriorSigma = \"prior thing\",\n",
    "                 fluxLuminosityNorm = \"prior thing\",\n",
    "                 alpha_C = \"prior thing\",\n",
    "                 V_C = \"prior thing\",\n",
    "                 alpha_L = \"prior thing\",\n",
    "                 V_L = \"prior thing\",\n",
    "                 lineWidthSigma = \"prior thing\",\n",
    "                 inform_options = \"inform options\")\n",
    "\n",
    "\n",
    "class delightPZ(BaseEstimation):\n",
    "\n",
    "    def __init__(self, base_config, config_dict='None'):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        run_dict: dict\n",
    "          dictionary of all variables read in from the run_params\n",
    "          values in the yaml file\n",
    "        \"\"\"\n",
    "        if config_dict == \"None\":\n",
    "            print(\"No config file supplied, using default parameters\")\n",
    "            config_dict = def_param\n",
    "        config_dict = check_and_print_params(config_dict, def_param,desc_dict)\n",
    "        \n",
    "        \n",
    "        super().__init__(base_config=base_config, config_dict=config_dict)\n",
    "\n",
    "        inputs = self.config_dict['run_params']\n",
    "\n",
    "        self.zmin = inputs['dlght_redshiftMin']\n",
    "        if self.zmin <= 0.:\n",
    "            raise ValueError(\"zmin must be greater than zero!\"\n",
    "                             + \"set dlght_redshiftMin accordingly\")\n",
    "        self.zmax = inputs['dlght_redshiftMax']\n",
    "        self.width = inputs['dlght_redshiftBinSize']\n",
    "        self.zgrid = np.arange(self.zmin, self.zmax, self.width) # this is how createGrids defines the grid\n",
    "        self.nzbins = len(self.zgrid)\n",
    "        \n",
    "        # temporary directories for Delight temprary file\n",
    "        self.tempdir = inputs['tempdir']\n",
    "        self.tempdatadir = inputs['tempdatadir']\n",
    "        self.sed_path = inputs['sed_path']\n",
    "        self.bands_path = inputs['bands_path']\n",
    "        # name of delight configuration file\n",
    "        self.delightparamfile=inputs[\"delightparamfile\"]\n",
    "        self.delightparamfile = os.path.join(self.tempdir, self.delightparamfile)\n",
    "\n",
    "        self.delightindata=inputs['dlght_inputdata']\n",
    "\n",
    "        # Choice of the running mode\n",
    "        self.tutorialmode = inputs[\"dlght_tutorialmode\"]\n",
    "        self.tutorialpasseval = False  # onmy one chunk for simulation\n",
    "        # for standard mode with DC2 dataset\n",
    "        self.flag_filter_training = inputs[\"flag_filter_training\"]\n",
    "        self.snr_cut_training = inputs[\"snr_cut_training\"]\n",
    "        self.flag_filter_validation = inputs[\"flag_filter_validation\"]\n",
    "        self.snr_cut_validation = inputs[\"snr_cut_validation\"]\n",
    "\n",
    "        self.sed_path = inputs['sed_path']\n",
    "        self.sed_name_list = inputs['sed_name_list']\n",
    "        self.sed_fmt = inputs['sed_fmt']\n",
    "        self.prior_t_list = inputs['prior_t_list']\n",
    "        self.prior_zt_list = inputs['prior_zt_list']\n",
    "        self.lambda_ref = inputs['lambda_ref']\n",
    "        \n",
    "\n",
    "        # counter on the chunk validation dataset\n",
    "        self.chunknum=0\n",
    "\n",
    "        self.dlght_calibrateTemplateMixturePrior = inputs[\"dlght_calibrateTemplateMixturePrior\"]\n",
    "        # all parameter files\n",
    "        self.inputs = inputs\n",
    "\n",
    "        np.random.seed(87)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b0d0b2",
   "metadata": {},
   "source": [
    "## Training (~Delight learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1da0e27",
   "metadata": {},
   "source": [
    "### delightPZ  inform call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab15365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "   def inform(self, training_data, hyperParamName=\"\", hyperParamList=[]):\n",
    "        \"\"\"\n",
    "          this is delightPZ\n",
    "        \"\"\"\n",
    "        \n",
    "        msg = \" INFORM \"\n",
    "        logger.info(msg)\n",
    "        logger.info(\"Try to workout filters\")\n",
    "        \n",
    "        # create usefull tempory directory\n",
    "        # --------------------------------\n",
    "        try:\n",
    "            if not os.path.exists(self.tempdir):\n",
    "                os.makedirs(self.tempdir)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                msg = \"error creating file \"+self.tempdir\n",
    "                logger.error(msg)\n",
    "                raise\n",
    "\n",
    "        try:\n",
    "            if not os.path.exists(self.tempdatadir):\n",
    "                os.makedirs(self.tempdatadir)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                msg = \"error creating file \" + self.tempdatadir\n",
    "                logger.error(msg)\n",
    "                raise\n",
    "        \n",
    "        # Very awfull way to get the internal data path for Delight\n",
    "        # This is the delight setup.py tools that get the data there\n",
    "        #basedelight_datapath = resource_filename('delight', '../data')\n",
    "        basedelight_datapath = self.delightindata\n",
    "          \n",
    "        # create parameter file\n",
    "        \n",
    "        logger.debug(\"Guessed basedelight_datapath  = \" + basedelight_datapath )\n",
    "\n",
    "        # gives to Delight where are its datapath and provide yaml arguments\n",
    "        paramfile_txt=makeConfigParam(basedelight_datapath, self.inputs)\n",
    "        logger.debug(paramfile_txt)\n",
    "\n",
    "\n",
    "        # save the config  parameter file that Delight needs\n",
    "        with open(self.delightparamfile, 'w') as out:\n",
    "            out.write(paramfile_txt)\n",
    "            \n",
    "            \n",
    "        # The data required by Delight : SED and Filters \n",
    "        # For the moment, there is no automatic installation inside RAIL\n",
    "        # These must be installed by the user, later these will be copied from Delight installation automatically\n",
    "\n",
    "        #if not os.path.exists(self.delightindata):\n",
    "        #    msg = \" No Delight input data in dir  \" + self.delightindata\n",
    "        #    logger.error(msg)\n",
    "        #    exit(-1)\n",
    "\n",
    "        #SUBDIRs = ['BROWN_SEDs', 'CWW_SEDs', 'FILTERS']\n",
    "\n",
    "        #for subdir in SUBDIRs:\n",
    "        #    theinpath=os.path.join(self.delightindata,subdir)\n",
    "        #    if not os.path.exists(theinpath):\n",
    "        #        msg = \" No Delight input data in dir  \" + theinpath\n",
    "        #        logger.error(msg)\n",
    "        #        exit(-1)\n",
    "        delightparamfile=\"tmp/parametersTest.cfg\"\n",
    "        \n",
    "        sed_path=\n",
    "        \n",
    "        if not os.path.exists(self.sed_path):\n",
    "            msg = \" No Delight SED data in dir \" + self.sed_path\n",
    "            logger.error(msg)\n",
    "            exit(-1)\n",
    "        if not os.path.exists(self.bands_path):\n",
    "            msg = \" No Delight FILTER data in dir \" + self.bands_path\n",
    "            logger.error(msg)\n",
    "            exit(-1)\n",
    "\n",
    "        # Initialisation of Delight with 1) Filters 2) SED to get Flux-redshift model \n",
    "\n",
    "        # Build LSST filter model\n",
    "        processFilters(self.delightparamfile)\n",
    "\n",
    "        # Build its own LSST-Flux-Redshift Model\n",
    "        processSEDs(self.delightparamfile)\n",
    "\n",
    "        if self.tutorialmode:\n",
    "            # Delight build its own Mock simulations\n",
    "            simulateWithSEDs(self.delightparamfile)\n",
    "\n",
    "        else:  # convert training_data  into ascii in desc input mode\n",
    "            convertDESCcatTrainData(self.delightparamfile,\\\n",
    "                                    training_data,flag_filter=self.flag_filter_training,\\\n",
    "                                    snr_cut=self.snr_cut_training)\n",
    "            \n",
    "            # convert target Files into ascii \n",
    "            # Delight need to know this target file\n",
    "            convertDESCcatTargetFile(self.delightparamfile,self.testfile,\\\n",
    "                                flag_filter=self.flag_filter_validation,\\\n",
    "                                     snr_cut=self.snr_cut_validation)\n",
    "            \n",
    "            if self.dlght_calibrateTemplateMixturePrior:\n",
    "                calibrateTemplateMixturePriors(self.delightparamfile)\n",
    "        \n",
    "        # Learn with Gaussian processes\n",
    "        delightLearn(self.delightparamfile, hyperParam_name=hyperParamName, hyperParam_list=hyperParamList)\n",
    "        \n",
    "        logger.info(\"End of Inform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e450df94",
   "metadata": {},
   "source": [
    "### Fonction learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a5ba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delightLearn(configfilename, hyperParam_name=\"\", hyperParam_list=[]):\n",
    "    \"\"\"\n",
    "\n",
    "    :param configfilename:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    threadNum = 0\n",
    "    numThreads = 1\n",
    "\n",
    "    #parse arguments\n",
    "\n",
    "    params = parseParamFile(configfilename, verbose=False)\n",
    "\n",
    "    if threadNum == 0:\n",
    "        logger.info(\"--- DELIGHT-LEARN ---\")\n",
    "\n",
    "    # Read filter coefficients, compute normalization of filters\n",
    "    bandCoefAmplitudes, bandCoefPositions, bandCoefWidths, norms = readBandCoefficients(params)\n",
    "    numBands = bandCoefAmplitudes.shape[0]\n",
    "\n",
    "    redshiftDistGrid, redshiftGrid, redshiftGridGP = createGrids(params)\n",
    "\n",
    "    f_mod = readSEDs(params)\n",
    "\n",
    "    numObjectsTraining = np.sum(1 for line in open(params['training_catFile']))\n",
    "\n",
    "    msg= 'Number of Training Objects ' + str(numObjectsTraining)\n",
    "    logger.info(msg)\n",
    "\n",
    "\n",
    "    firstLine = int(threadNum * numObjectsTraining / numThreads)\n",
    "    lastLine = int(min(numObjectsTraining,(threadNum + 1) * numObjectsTraining / numThreads))\n",
    "    numLines = lastLine - firstLine\n",
    "\n",
    "    msg ='Thread ' +  str(threadNum) + ' , analyzes lines ' + str(firstLine) + ' , to ' + str(lastLine)\n",
    "    logger.info(msg)\n",
    "\n",
    "    DL = approx_DL()\n",
    "    gp = PhotozGP(f_mod, bandCoefAmplitudes, bandCoefPositions, bandCoefWidths,\n",
    "              params['lines_pos'], params['lines_width'],\n",
    "              params['V_C'], params['V_L'],\n",
    "              params['alpha_C'], params['alpha_L'],\n",
    "              redshiftGridGP, use_interpolators=True)\n",
    "\n",
    "    B = numBands\n",
    "    numCol = 3 + B + B*(B+1)//2 + B + f_mod.shape[0]\n",
    "    localData = np.zeros((numLines, numCol))\n",
    "    fmt = '%i ' + '%.12e ' * (localData.shape[1] - 1)\n",
    "\n",
    "    loc = - 1\n",
    "    crossValidate = params['training_crossValidate']\n",
    "    trainingDataIter1 = getDataFromFile(params, firstLine, lastLine,prefix=\"training_\", getXY=True,CV=crossValidate)\n",
    "\n",
    "\n",
    "    if crossValidate:\n",
    "        chi2sLocal = None\n",
    "        bandIndicesCV, bandNamesCV, bandColumnsCV,bandVarColumnsCV, redshiftColumnCV = readColumnPositions(params, prefix=\"training_CV_\", refFlux=False)\n",
    "\n",
    "    if sensitivity and hyperParam_name != \"ellSigmaPrior\":\n",
    "        margLike_list=[]\n",
    "        abscissa_list=[]\n",
    "        for hyperParam in hyperParam_list:\n",
    "            if hyperParam_name == \"V_C\":\n",
    "                gp = PhotozGP(f_mod_interp,\n",
    "                          bandCoefAmplitudes, bandCoefPositions, bandCoefWidths,\n",
    "                          params['lines_pos'], params['lines_width'],\n",
    "                          hyperParam, params['V_L'],\n",
    "                          params['alpha_C'], params['alpha_L'],\n",
    "                          redshiftGridGP, use_interpolators=True)\n",
    "            elif hyperParam_name == \"V_L\":\n",
    "                gp = PhotozGP(f_mod_interp,\n",
    "                          bandCoefAmplitudes, bandCoefPositions, bandCoefWidths,\n",
    "                          params['lines_pos'], params['lines_width'],\n",
    "                          params['V_C'], hyperParam,\n",
    "                          params['alpha_C'], params['alpha_L'],\n",
    "                          redshiftGridGP, use_interpolators=True)\n",
    "            elif hyperParam_name == \"alpha_C\":\n",
    "                gp = PhotozGP(f_mod_interp,\n",
    "                          bandCoefAmplitudes, bandCoefPositions, bandCoefWidths,\n",
    "                          params['lines_pos'], params['lines_width'],\n",
    "                          params['V_C'], params['V_L'],\n",
    "                          hyperParam, params['alpha_L'],\n",
    "                          redshiftGridGP, use_interpolators=True)\n",
    "            elif hyperParam_name == \"alpha_L\":\n",
    "                gp = PhotozGP(f_mod_interp,\n",
    "                          bandCoefAmplitudes, bandCoefPositions, bandCoefWidths,\n",
    "                          params['lines_pos'], params['lines_width'],\n",
    "                          params['V_C'], params['V_L'],\n",
    "                          params['alpha_C'], hyperParam,\n",
    "                          redshiftGridGP, use_interpolators=True)\n",
    "\n",
    "            for z, normedRefFlux,\\\n",
    "                bands, fluxes, fluxesVar,\\\n",
    "                bandsCV, fluxesCV, fluxesVarCV,\\\n",
    "                X, Y, Yvar in trainingDataIter1:\n",
    "\n",
    "                loc += 1\n",
    "\n",
    "                themod = np.zeros((1, f_mod.shape[0], bands.size))\n",
    "                for it in range(f_mod.shape[0]):\n",
    "                    for ib, band in enumerate(bands):\n",
    "                        themod[0, it, ib] = f_mod[it, band](z)\n",
    "\n",
    "                # really calibrate the luminosity parameter l compared to the model\n",
    "                # according the best type of galaxy\n",
    "                chi2_grid, ellMLs = scalefree_flux_likelihood(fluxes,fluxesVar,themod,returnChi2=True)\n",
    "\n",
    "                bestType = np.argmin(chi2_grid)  # best type\n",
    "                ell = ellMLs[0, bestType]        # the luminosity factor\n",
    "                X[:, 2] = ell\n",
    "\n",
    "                gp.setData(X, Y, Yvar, bestType)\n",
    "\n",
    "                ### CALCUL MARGINAL LIKELIHODD OU AUTRE METRIQUE ###\n",
    "                margLike_list.append(gp.margLike())\n",
    "                abscissa_list.append(hyperParam)\n",
    "\n",
    "        ### PLOT METRIQUE ###\n",
    "        ## Plot for this iteration on ellPriorSigma:\n",
    "        alpha = 0.9\n",
    "        s = 5\n",
    "        fig, ax = plt.subplots(constrained_layout=True)\n",
    "\n",
    "        vs = ax.hist2d(abscissa_list, margLike_list, bins=[100, 100],\\\n",
    "                       range=[[np.min(abscissa_list), np.max(abscissa_list)], [-10, 200]],\\\n",
    "                       density=True, cmap=\"Reds\", alpha=alpha)\n",
    "\n",
    "        ax.set_xlabel(hyperParam_name)\n",
    "        ax.set_ylabel('GP Marginal likelihood')\n",
    "        ax.set_title('Effect of hyperparameter '+hyperParam_name+' on GP marginal likelihood during training process.')\n",
    "        fig.show()\n",
    "        \n",
    "    else:\n",
    "        for z, normedRefFlux,\\\n",
    "            bands, fluxes, fluxesVar,\\\n",
    "            bandsCV, fluxesCV, fluxesVarCV,\\\n",
    "            X, Y, Yvar in trainingDataIter1:\n",
    "\n",
    "            loc += 1\n",
    "\n",
    "            themod = np.zeros((1, f_mod.shape[0], bands.size))\n",
    "            for it in range(f_mod.shape[0]):\n",
    "                for ib, band in enumerate(bands):\n",
    "                    themod[0, it, ib] = f_mod[it, band](z)\n",
    "\n",
    "            # really calibrate the luminosity parameter l compared to the model\n",
    "            # according the best type of galaxy\n",
    "            chi2_grid, ellMLs = scalefree_flux_likelihood(fluxes,fluxesVar,themod,returnChi2=True)\n",
    "\n",
    "            bestType = np.argmin(chi2_grid)  # best type\n",
    "            ell = ellMLs[0, bestType]        # the luminosity factor\n",
    "            X[:, 2] = ell\n",
    "\n",
    "            gp.setData(X, Y, Yvar, bestType)\n",
    "            lB = bands.size\n",
    "            localData[loc, 0] = lB\n",
    "            localData[loc, 1] = z\n",
    "            localData[loc, 2] = ell\n",
    "            localData[loc, 3:3+lB] = bands\n",
    "            localData[loc, 3+lB:3+f_mod.shape[0]+lB+lB*(lB+1)//2+lB] = gp.getCore()\n",
    "\n",
    "            if crossValidate:\n",
    "                model_mean, model_covar = gp.predictAndInterpolate(np.array([z]), ell=ell)\n",
    "                if chi2sLocal is None:\n",
    "                    chi2sLocal = np.zeros((numObjectsTraining, bandIndicesCV.size))\n",
    "\n",
    "                ind = np.array([list(bandIndicesCV).index(b) for b in bandsCV])\n",
    "\n",
    "                chi2sLocal[firstLine + loc, ind] = - 0.5 * (model_mean[0, bandsCV] - fluxesCV)**2 /(model_covar[0, bandsCV] + fluxesVarCV)\n",
    "\n",
    "    if threadNum == 0:\n",
    "        reducedData = np.zeros((numObjectsTraining, numCol))\n",
    "    else:\n",
    "        reducedData = None\n",
    "\n",
    "    if crossValidate:\n",
    "        chi2sGlobal = np.zeros_like(chi2sLocal)\n",
    "        chi2sGlobal = chi2sLocal\n",
    "\n",
    "    firstLines = [int(k*numObjectsTraining/numThreads) for k in range(numThreads)]\n",
    "    lastLines = [int(min(numObjectsTraining, (k+1)*numObjectsTraining/numThreads)) for k in range(numThreads)]\n",
    "    sendcounts = tuple([(lastLines[k] - firstLines[k]) * numCol for k in range(numThreads)])\n",
    "    displacements = tuple([firstLines[k] * numCol for k in range(numThreads)])\n",
    "\n",
    "    reducedData = localData\n",
    "\n",
    "    # parameters for the GP process on traniing data are transfered to reduced data and saved in file\n",
    "    #'training_paramFile'\n",
    "    if threadNum == 0:\n",
    "        np.savetxt(params['training_paramFile'], reducedData, fmt=fmt)\n",
    "        if crossValidate:\n",
    "            np.savetxt(params['training_CVfile'], chi2sGlobal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ff9ab9",
   "metadata": {},
   "source": [
    "## Targets (~Delight apply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2098b26c",
   "metadata": {},
   "source": [
    "### delightPZ Estimate call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7074e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def estimate(self, test_data, hyperParamName=\"\", hyperParamList=[]):\n",
    "        \n",
    "        print(\"\\n\\n\\n Starting estimation...\\n\\n\\n\")\n",
    "        self.chunknum += 1\n",
    "\n",
    "        msg = \" ESTIMATE : chunk number {} \".format(self.chunknum)\n",
    "        logger.info(msg)\n",
    "\n",
    "    \n",
    "        basedelight_datapath = self.delightindata\n",
    "\n",
    "        msg=\" Delight input data file are in dir : {} \".format(self.delightparamfile)\n",
    "        logger.debug(msg)\n",
    "\n",
    "        # when Delight runs in tutorial mode call only once delightApply\n",
    "        if  self.tutorialmode and not self.tutorialpasseval:\n",
    "\n",
    "            msg = \"TUTORIAL MODE : process chunk {}\".format(self.chunknum)\n",
    "            logger.info(msg)\n",
    "\n",
    "            # Template Fitting\n",
    "            templateFitting(self.delightparamfile)\n",
    "\n",
    "            # Gaussian process fitting\n",
    "            delightApply(self.delightparamfile, hyperParam_name=hyperParamName, hyperParam_list=hyperParamList)\n",
    "            self.tutorialpasseval = True    # avoid latter call to delightApply when running in tutorial mode\n",
    "\n",
    "        elif self.tutorialmode and self.tutorialpasseval:\n",
    "            msg=\"TUTORIAL MODE : skip chunk {}\".format(self.chunknum)\n",
    "            logger.info(msg)\n",
    "\n",
    "        elif not self.tutorialmode : # let rail split the test data into chunks\n",
    "            msg = \"STANDARD MODE : process chunk {}\".format(self.chunknum)\n",
    "            logger.info(msg)\n",
    "\n",
    "            # Generate a new parameter file for delight this chunk\n",
    "            paramfile_txt=makeConfigParam(basedelight_datapath, self.inputs, self.chunknum)\n",
    "\n",
    "            # generate the config-parameter filename from chunk number\n",
    "            delightparamfile=self.delightparamfile\n",
    "            logger.debug(delightparamfile)\n",
    "            dirn=os.path.dirname(delightparamfile)\n",
    "            basn=os.path.basename(delightparamfile)\n",
    "            basnsplit=basn.split(\".\")\n",
    "            basnchunk =  basnsplit[0] + \"_\" + str(self.chunknum) + \".\" + basnsplit[1]\n",
    "            delightparamfilechunk = os.path.join(dirn,basnchunk)\n",
    "            logger.debug(\"parameter file for delight :\" + delightparamfilechunk)\n",
    "\n",
    "            # save the config parameter file for the data chunk that Delight needs\n",
    "            with open(delightparamfilechunk, 'w') as out:\n",
    "                out.write(paramfile_txt)\n",
    "\n",
    "            # convert the chunk data into the required  flux-redshift validation file for delight\n",
    "            indexes_sel=convertDESCcatChunk(delightparamfilechunk, test_data, self.chunknum,\\\n",
    "                                flag_filter_validation=self.flag_filter_validation,\\\n",
    "                                snr_cut_validation=self.snr_cut_validation)\n",
    "\n",
    "            # template fitting for that chunk\n",
    "            templateFitting(delightparamfilechunk)\n",
    "\n",
    "            # estimation for that chunk\n",
    "            delightApply(delightparamfilechunk, hyperParam_name=hyperParamName, hyperParam_list=hyperParamList)\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            msg = \"STANDARD MODE : pass chunk {}\".format(self.chunknum)\n",
    "            logger.info(msg)\n",
    "     \n",
    "        \n",
    "        pdf = []\n",
    "        # allow for either format for now\n",
    "        try:\n",
    "            d = test_data['i_mag']\n",
    "        except Exception:\n",
    "            d = test_data['mag_i_lsst']\n",
    "\n",
    "        numzs = len(d)\n",
    "\n",
    "        if self.tutorialmode:\n",
    "            # fill creazy values (simulation not send to rail)\n",
    "            zmode = np.round(np.random.uniform(self.zmin, self.zmax, numzs), 3)\n",
    "            pdfs = np.zeros([numzs, len(self.zgrid)])\n",
    "            for i in range(numzs):\n",
    "                pdfs[i] = (norm.pdf(self.zgrid, zmode[i], self.width * (1 + zmode[i])))\n",
    "\n",
    "        else:\n",
    "            zmode, pdfs = \\\n",
    "            getDelightRedshiftEstimation(delightparamfilechunk,self.chunknum,numzs,indexes_sel)\n",
    "            zmode = np.round(zmode,3)\n",
    "\n",
    "\n",
    "        if self.output_format == 'qp':\n",
    "            qp_d = qp.Ensemble(qp.interp, data=dict(xvals=self.zgrid,\n",
    "                                                    yvals=pdfs))\n",
    "            return qp_d\n",
    "        else:\n",
    "            pz_dict = {'zmode': zmode, 'pz_pdf': pdfs}\n",
    "            return pz_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fa6c4b",
   "metadata": {},
   "source": [
    "### ~Template fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae20e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def templateFitting(configfilename):\n",
    "    \"\"\"\n",
    "\n",
    "    :param configfilename:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(f\"\\n\\n\\n\\n Templatefitting: using configfile:{configfilename}\")\n",
    "    \n",
    "    threadNum = 0\n",
    "    numThreads = 1\n",
    "\n",
    "    if threadNum == 0:\n",
    "        logger.info(\"--- TEMPLATE FITTING ---\")\n",
    "\n",
    "        if FLAG_NEW_PRIOR:\n",
    "            logger.info(\"==> New Prior calculation from Benitez\")\n",
    "\n",
    "    # Parse parameters file\n",
    "\n",
    "    paramFileName = configfilename\n",
    "    params = parseParamFile(paramFileName, verbose=False)\n",
    "\n",
    "    if threadNum == 0:\n",
    "        msg = 'Thread number / number of threads: ' + str(threadNum+1) + \" , \" + str(numThreads)\n",
    "        logger.info(msg)\n",
    "        msg = 'Input parameter file:' + paramFileName\n",
    "        logger.info(msg)\n",
    "\n",
    "\n",
    "\n",
    "    DL = approx_DL()\n",
    "    redshiftDistGrid, redshiftGrid, redshiftGridGP = createGrids(params)\n",
    "    numZ = redshiftGrid.size\n",
    "\n",
    "    # Locate which columns of the catalog correspond to which bands.\n",
    "\n",
    "    bandIndices, bandNames, bandColumns, bandVarColumns, redshiftColumn,refBandColumn = readColumnPositions(params, prefix=\"target_\")\n",
    "\n",
    "    dir_seds = params['templates_directory']\n",
    "    dir_filters = params['bands_directory']\n",
    "    lambdaRef = params['lambdaRef']\n",
    "    sed_names = params['templates_names']\n",
    "\n",
    "    # f_mod  : flux model in each band as a function of the sed and the band name\n",
    "    # axis 0 : redshifts\n",
    "    # axis 1 : sed names\n",
    "    # axis 2 : band names\n",
    "\n",
    "    f_mod = np.zeros((redshiftGrid.size, len(sed_names),len(params['bandNames'])))\n",
    "\n",
    "    # loop on SED to load the flux-redshift file from the training\n",
    "    # ture data or simulated by simulateWithSEDs.py\n",
    "\n",
    "    for t, sed_name in enumerate(sed_names):\n",
    "        f_mod[:, t, :] = np.loadtxt(dir_seds + '/' + sed_name + '_fluxredshiftmod.txt')\n",
    "\n",
    "    numObjectsTarget = np.sum(1 for line in open(params['target_catFile']))\n",
    "\n",
    "    firstLine = int(threadNum * numObjectsTarget / float(numThreads))\n",
    "    lastLine = int(min(numObjectsTarget,(threadNum + 1) * numObjectsTarget / float(numThreads)))\n",
    "    numLines = lastLine - firstLine\n",
    "\n",
    "    if threadNum == 0:\n",
    "        msg='Number of Target Objects ' + str(numObjectsTarget)\n",
    "        logger.info(msg)\n",
    "\n",
    "    msg= 'Thread ' + str(threadNum) + ' , analyzes lines ' + str(firstLine) +  ' , to ' +  str(lastLine)\n",
    "    logger.info(msg)\n",
    "\n",
    "    numMetrics = 7 + len(params['confidenceLevels'])\n",
    "\n",
    "    # Create local files to store results\n",
    "    localPDFs = np.zeros((numLines, numZ))\n",
    "    localMetrics = np.zeros((numLines, numMetrics))\n",
    "\n",
    "    # Now loop over each target galaxy (indexed bu loc index) to compute likelihood function\n",
    "    # with its flux in each bands\n",
    "    loc = - 1\n",
    "    trainingDataIter = getDataFromFile(params, firstLine, lastLine,prefix=\"target_\", getXY=False)\n",
    "    for z, normedRefFlux, bands, fluxes, fluxesVar,bCV, fCV, fvCV in trainingDataIter:\n",
    "        loc += 1\n",
    "        # like_grid, _ = scalefree_flux_likelihood(\n",
    "        #    fluxes, fluxesVar,\n",
    "        #    f_mod[:, :, bands])\n",
    "        # ell_hat_z = normedRefFlux * 4 * np.pi\\\n",
    "        #    * params['fluxLuminosityNorm'] \\\n",
    "        #    * (DL(redshiftGrid)**2. * (1+redshiftGrid))[:, None]\n",
    "\n",
    "        # OLD way be keep it now\n",
    "        ell_hat_z = 1\n",
    "        params['ellPriorSigma'] = 1e12\n",
    "\n",
    "        # Not working\n",
    "        #ell_hat_z=0.45e-4\n",
    "        #params['ellPriorSigma'] = 1e12\n",
    "\n",
    "        # approximate flux likelihood, with scaling of both the mean and variance.\n",
    "        # This approximates the true likelihood with an iterative scheme.\n",
    "        # - data : fluxes, fluxesVar\n",
    "        # - model based on SED : f_mod\n",
    "        like_grid = approx_flux_likelihood(fluxes, fluxesVar, f_mod[:, :, bands],normalized=True, marginalizeEll=True,ell_hat=ell_hat_z, ell_var=(ell_hat_z*params['ellPriorSigma'])**2)\n",
    "\n",
    "        if FLAG_NEW_PRIOR:\n",
    "            maglim=26  # M5 magnitude max\n",
    "            p_z = libPriorPZ(redshiftGrid,maglim=maglim)  # return 2D template nz x nt, nt is 8\n",
    "\n",
    "\n",
    "        else:\n",
    "            b_in = np.array(params['p_t'])[None, :]\n",
    "            beta2 = np.array(params['p_z_t'])**2.0\n",
    "\n",
    "            #compute prior on z\n",
    "            p_z = b_in * redshiftGrid[:, None] / beta2[None, :] *np.exp(-0.5 * redshiftGrid[:, None]**2 / beta2[None, :])\n",
    "\n",
    "        if loc < 0:\n",
    "            np.set_printoptions(threshold=20, edgeitems=10, linewidth=140,formatter=dict(float=lambda x: \"%.3e\" % x))  # float arrays %.3g\n",
    "            print(p_z)\n",
    "\n",
    "        # Compute likelihood x prior\n",
    "        like_grid *= p_z\n",
    "\n",
    "        localPDFs[loc, :] += like_grid.sum(axis=1)\n",
    "    \n",
    "        if localPDFs[loc, :].sum() > 0:\n",
    "            localMetrics[loc, :] = computeMetrics(z, redshiftGrid,localPDFs[loc, :],params['confidenceLevels'])\n",
    "\n",
    "    if threadNum == 0:\n",
    "        globalPDFs = np.zeros((numObjectsTarget, numZ))\n",
    "        globalMetrics = np.zeros((numObjectsTarget, numMetrics))\n",
    "    else:\n",
    "        globalPDFs = None\n",
    "        globalMetrics = None\n",
    "\n",
    "    firstLines = [int(k*numObjectsTarget/numThreads) for k in range(numThreads)]\n",
    "    lastLines = [int(min(numObjectsTarget, (k+1)*numObjectsTarget/numThreads)) for k in range(numThreads)]\n",
    "    numLines = [lastLines[k] - firstLines[k] for k in range(numThreads)]\n",
    "\n",
    "    sendcounts = tuple([numLines[k] * numZ for k in range(numThreads)])\n",
    "    displacements = tuple([firstLines[k] * numZ for k in range(numThreads)])\n",
    "    globalPDFs = localPDFs\n",
    "\n",
    "\n",
    "    sendcounts = tuple([numLines[k] * numMetrics for k in range(numThreads)])\n",
    "    displacements = tuple([firstLines[k] * numMetrics for k in range(numThreads)])\n",
    "    globalMetrics = localMetrics \n",
    "\n",
    "    if threadNum == 0:\n",
    "        fmt = '%.2e'\n",
    "        np.savetxt(params['redshiftpdfFileTemp'], globalPDFs, fmt=fmt)\n",
    "        if redshiftColumn >= 0:\n",
    "            np.savetxt(params['metricsFileTemp'], globalMetrics, fmt=fmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469192ac",
   "metadata": {},
   "source": [
    "### ~Delight apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d319cb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delightApply(configfilename, hyperParam_name=\"\", hyperParam_list=[]):\n",
    "    \"\"\"\n",
    "\n",
    "    :param configfilename:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    from astropy.visualization import hist\n",
    "    # Sensitivity study or normal run?\n",
    "    bool sensitivity = False\n",
    "    if (hyperParam_name!=\"V_C\" and hyperParam_name!=\"V_L\" and hyperParam_name!=\"alpha_C\" \\\n",
    "        and hyperParam_name!=\"alpha_L\" and hyperParam_name!=\"ellSigmaPrior\") or len(hyperParam_list)<1 :\n",
    "        print(\"Invalid hyperparameter for sensitivity study. Running with default values.\")\n",
    "    else:\n",
    "        print(\"Running sensitivity study for hyperparameter \"+hyperParam_name+\" .\")\n",
    "        sensitivity=True\n",
    "\n",
    "    threadNum = 0\n",
    "    numThreads = 1\n",
    "\n",
    "    params = parseParamFile(configfilename, verbose=False)\n",
    "\n",
    "    if threadNum == 0:\n",
    "        logger.info(\"--- DELIGHT-APPLY ---\")\n",
    "\n",
    "    # Read filter coefficients, compute normalization of filters\n",
    "    bandCoefAmplitudes, bandCoefPositions, bandCoefWidths, norms = readBandCoefficients(params)\n",
    "    numBands = bandCoefAmplitudes.shape[0]\n",
    "\n",
    "    redshiftDistGrid, redshiftGrid, redshiftGridGP = createGrids(params)\n",
    "    f_mod_interp = readSEDs(params)\n",
    "    nt = f_mod_interp.shape[0]\n",
    "    nz = redshiftGrid.size\n",
    "\n",
    "    dir_seds = params['templates_directory']\n",
    "    dir_filters = params['bands_directory']\n",
    "    lambdaRef = params['lambdaRef']\n",
    "    sed_names = params['templates_names']\n",
    "    f_mod_grid = np.zeros((redshiftGrid.size, len(sed_names),len(params['bandNames'])))\n",
    "\n",
    "    for t, sed_name in enumerate(sed_names):\n",
    "        f_mod_grid[:, t, :] = np.loadtxt(dir_seds + '/' + sed_name +'_fluxredshiftmod.txt')\n",
    "\n",
    "    numZbins = redshiftDistGrid.size - 1\n",
    "    numZ = redshiftGrid.size\n",
    "\n",
    "    numObjectsTraining = np.sum(1 for line in open(params['training_catFile']))\n",
    "    numObjectsTarget = np.sum(1 for line in open(params['target_catFile']))\n",
    "    redshiftsInTarget = ('redshift' in params['target_bandOrder'])\n",
    "    Ncompress = params['Ncompress']\n",
    "\n",
    "    firstLine = int(threadNum * numObjectsTarget / float(numThreads))\n",
    "    lastLine = int(min(numObjectsTarget,(threadNum + 1) * numObjectsTarget / float(numThreads)))\n",
    "    numLines = lastLine - firstLine\n",
    "\n",
    "    if threadNum == 0:\n",
    "        msg= 'Number of Training Objects ' +  str(numObjectsTraining)\n",
    "        logger.info(msg)\n",
    "\n",
    "        msg='Number of Target Objects ' + str(numObjectsTarget)\n",
    "        logger.info(msg)\n",
    "\n",
    "    msg= 'Thread '+ str(threadNum) + ' , analyzes lines ' +  str(firstLine) + ' to ' + str( lastLine)\n",
    "    logger.info(msg)\n",
    "\n",
    "    DL = approx_DL()\n",
    "\n",
    "    # Create local files to store results\n",
    "    numMetrics = 7 + len(params['confidenceLevels'])\n",
    "    localPDFs = np.zeros((numLines, numZ))\n",
    "    localMetrics = np.zeros((numLines, numMetrics))\n",
    "    localCompressIndices = np.zeros((numLines,  Ncompress), dtype=int)\n",
    "    localCompEvidences = np.zeros((numLines,  Ncompress))\n",
    "\n",
    "    # Looping over chunks of the training set to prepare model predictions over\n",
    "    if sensitivity:\n",
    "        numChunks = 1\n",
    "    else:\n",
    "        numChunks = params['training_numChunks']\n",
    "        \n",
    "    if sensitivity and hyperParam_name != \"ellSigmaPrior\":\n",
    "        margLike_list=[]\n",
    "        abscissa_list=[]\n",
    "        for chunk in range(numChunks):\n",
    "            TR_firstLine = int(chunk * numObjectsTraining / float(numChunks))\n",
    "            TR_lastLine = int(min(numObjectsTraining, (chunk + 1) * numObjectsTarget / float(numChunks)))\n",
    "            targetIndices = np.arange(TR_firstLine, TR_lastLine)\n",
    "            numTObjCk = TR_lastLine - TR_firstLine\n",
    "            redshifts = np.zeros((numTObjCk, ))\n",
    "            model_mean = np.zeros((numZ, numTObjCk, numBands))\n",
    "            model_covar = np.zeros((numZ, numTObjCk, numBands))\n",
    "            bestTypes = np.zeros((numTObjCk, ), dtype=int)\n",
    "            ells = np.zeros((numTObjCk, ), dtype=int)\n",
    "\n",
    "            # loop on training data and training GP coefficients produced by delight_learn\n",
    "            # It fills the model_mean and model_covar predicted by GP\n",
    "            loc = TR_firstLine - 1\n",
    "            trainingDataIter = getDataFromFile(params, TR_firstLine, TR_lastLine,prefix=\"training_\", ftype=\"gpparams\")\n",
    "\n",
    "            for hyperParam in hyperParam_list:\n",
    "                if hyperParam_name == \"V_C\":\n",
    "                    gp = PhotozGP(f_mod_interp,\n",
    "                              bandCoefAmplitudes, bandCoefPositions, bandCoefWidths,\n",
    "                              params['lines_pos'], params['lines_width'],\n",
    "                              hyperParam, params['V_L'],\n",
    "                              params['alpha_C'], params['alpha_L'],\n",
    "                              redshiftGridGP, use_interpolators=True)\n",
    "                elif hyperParam_name == \"V_L\":\n",
    "                    gp = PhotozGP(f_mod_interp,\n",
    "                              bandCoefAmplitudes, bandCoefPositions, bandCoefWidths,\n",
    "                              params['lines_pos'], params['lines_width'],\n",
    "                              params['V_C'], hyperParam,\n",
    "                              params['alpha_C'], params['alpha_L'],\n",
    "                              redshiftGridGP, use_interpolators=True)\n",
    "                elif hyperParam_name == \"alpha_C\":\n",
    "                    gp = PhotozGP(f_mod_interp,\n",
    "                              bandCoefAmplitudes, bandCoefPositions, bandCoefWidths,\n",
    "                              params['lines_pos'], params['lines_width'],\n",
    "                              params['V_C'], params['V_L'],\n",
    "                              hyperParam, params['alpha_L'],\n",
    "                              redshiftGridGP, use_interpolators=True)\n",
    "                elif hyperParam_name == \"alpha_L\":\n",
    "                    gp = PhotozGP(f_mod_interp,\n",
    "                              bandCoefAmplitudes, bandCoefPositions, bandCoefWidths,\n",
    "                              params['lines_pos'], params['lines_width'],\n",
    "                              params['V_C'], params['V_L'],\n",
    "                              params['alpha_C'], hyperParam,\n",
    "                              redshiftGridGP, use_interpolators=True)\n",
    "\n",
    "                # loop on training data to load the GP parameter\n",
    "                for loc, (z, ell, bands, X, B, flatarray) in enumerate(trainingDataIter):\n",
    "                    t1 = time()\n",
    "                    redshifts[loc] = z              # redshift of all training samples\n",
    "                    gp.setCore(X, B, nt,flatarray[0:nt+B+B*(B+1)//2])\n",
    "                    bestTypes[loc] = gp.bestType   # retrieve the best-type found by delight-learn\n",
    "                    ells[loc] = ell                # retrieve the luminosity parameter l\n",
    "\n",
    "                    # here is the model prediction of Gaussian Process for that particular trainning galaxy\n",
    "                    model_mean[:, loc, :], model_covar[:, loc, :] = gp.predictAndInterpolate(redshiftGrid, ell=ell)\n",
    "                    t2 = time()\n",
    "                    # print(loc, t2-t1)\n",
    "\n",
    "                    ### CALCUL MARGINAL LIKELIHODD OU AUTRE METRIQUE ###\n",
    "                    margLike_list.append(gp.margLike())\n",
    "                    abscissa_list.append(hyperParam)\n",
    "\n",
    "            ### PLOT METRIQUE ###\n",
    "            ## Plot for this iteration on ellPriorSigma:\n",
    "            alpha = 0.9\n",
    "            s = 5\n",
    "            fig, ax = plt.subplots(constrained_layout=True)\n",
    "\n",
    "            vs = ax.hist2d(abscissa_list, margLike_list, bins=[100, 100],\\\n",
    "                           range=[[np.min(abscissa_list), np.max(abscissa_list)], [-10, 200]],\\\n",
    "                           density=True, cmap=\"Reds\", alpha=alpha)\n",
    "\n",
    "            ax.set_xlabel(hyperParam_name)\n",
    "            ax.set_ylabel('GP Marginal likelihood')\n",
    "            ax.set_title('Training chunk No {}'.format(chunk))\n",
    "            fig.suptitle('Effect of hyperparameter '+hyperParam_name+' on GP marginal likelihood during estimation process.')\n",
    "            fig.show()\n",
    "            \n",
    "            #Redshift prior on training galaxy\n",
    "            # p_t = params['p_t'][bestTypes][None, :]\n",
    "            # p_z_t = params['p_z_t'][bestTypes][None, :]\n",
    "            # compute the prior for taht training sample\n",
    "            prior = np.exp(-0.5*((redshiftGrid[:, None]-redshifts[None, :]) /params['zPriorSigma'])**2)\n",
    "            # prior[prior < 1e-6] = 0\n",
    "            # prior *= p_t * redshiftGrid[:, None] *\n",
    "            # np.exp(-0.5 * redshiftGrid[:, None]**2 / p_z_t) / p_z_t\n",
    "\n",
    "            if params['useCompression'] and params['compressionFilesFound']:\n",
    "                fC = open(params['compressMargLikFile'])\n",
    "                fCI = open(params['compressIndicesFile'])\n",
    "                itCompM = itertools.islice(fC, firstLine, lastLine)\n",
    "                iterCompI = itertools.islice(fCI, firstLine, lastLine)\n",
    "\n",
    "            targetDataIter = getDataFromFile(params, firstLine, lastLine,prefix=\"target_\", getXY=False, CV=False)\n",
    "\n",
    "            # loop on target samples\n",
    "            for loc, (z, normedRefFlux, bands, fluxes, fluxesVar, bCV, dCV, dVCV) in enumerate(targetDataIter):\n",
    "                t1 = time()\n",
    "                ell_hat_z = normedRefFlux * 4 * np.pi * params['fluxLuminosityNorm'] * (DL(redshiftGrid)**2. * (1+redshiftGrid))\n",
    "                ell_hat_z[:] = 1\n",
    "                if params['useCompression'] and params['compressionFilesFound']:\n",
    "                    indices = np.array(next(iterCompI).split(' '), dtype=int)\n",
    "                    sel = np.in1d(targetIndices, indices, assume_unique=True)\n",
    "                    # same likelihood as for template fitting\n",
    "                    like_grid2 = approx_flux_likelihood(fluxes,fluxesVar,model_mean[:, sel, :][:, :, bands],\n",
    "                    f_mod_covar=model_covar[:, sel, :][:, :, bands],\n",
    "                    marginalizeEll=True, normalized=False,\n",
    "                    ell_hat=ell_hat_z,\n",
    "                    ell_var=(ell_hat_z*params['ellPriorSigma'])**2)\n",
    "                    like_grid *= prior[:, sel]\n",
    "                else:\n",
    "                    like_grid = np.zeros((nz, model_mean.shape[1]))\n",
    "                    # same likelihood as for template fitting, but cython\n",
    "                    approx_flux_likelihood_cy(\n",
    "                        like_grid, nz, model_mean.shape[1], bands.size,\n",
    "                        fluxes, fluxesVar,  # target galaxy fluxes and variance\n",
    "                        model_mean[:, :, bands],     # prediction with Gaussian process\n",
    "                        model_covar[:, :, bands],\n",
    "                        ell_hat=ell_hat_z,           # it will find internally the ell\n",
    "                        ell_var=(ell_hat_z*params['ellPriorSigma'])**2)\n",
    "                    like_grid *= prior[:, :] #likelihood multiplied by redshift training galaxies priors\n",
    "                t2 = time()\n",
    "                localPDFs[loc, :] += like_grid.sum(axis=1)  # the final redshift posterior is sum over training galaxies posteriors\n",
    "\n",
    "                # compute the evidence for each model\n",
    "                evidences = np.trapz(like_grid, x=redshiftGrid, axis=0) ## EVIDENCE =? MARGINAL LIKELIHOOD\n",
    "                t3 = time()\n",
    "\n",
    "                if params['useCompression'] and not params['compressionFilesFound']:\n",
    "                    if localCompressIndices[loc, :].sum() == 0:\n",
    "                        sortind = np.argsort(evidences)[::-1][0:Ncompress]\n",
    "                        localCompressIndices[loc, :] = targetIndices[sortind]\n",
    "                        localCompEvidences[loc, :] = evidences[sortind]\n",
    "                    else:\n",
    "                        dind = np.concatenate((targetIndices,localCompressIndices[loc, :]))\n",
    "                        devi = np.concatenate((evidences,localCompEvidences[loc, :]))\n",
    "                        sortind = np.argsort(devi)[::-1][0:Ncompress]\n",
    "                        localCompressIndices[loc, :] = dind[sortind]\n",
    "                        localCompEvidences[loc, :] = devi[sortind]\n",
    "\n",
    "                if chunk == numChunks - 1\\\n",
    "                        and redshiftsInTarget\\\n",
    "                     and localPDFs[loc, :].sum() > 0:\n",
    "                    localMetrics[loc, :] = computeMetrics(z, redshiftGrid,localPDFs[loc, :],params['confidenceLevels'])\n",
    "                t4 = time()\n",
    "                if loc % 100 == 0:\n",
    "                    print(loc, t2-t1, t3-t2, t4-t3)\n",
    "\n",
    "            if params['useCompression'] and params['compressionFilesFound']:\n",
    "                fC.close()\n",
    "                fCI.close()\n",
    "                \n",
    "    else:\n",
    "        for chunk in range(numChunks):\n",
    "            TR_firstLine = int(chunk * numObjectsTraining / float(numChunks))\n",
    "            TR_lastLine = int(min(numObjectsTraining, (chunk + 1) * numObjectsTarget / float(numChunks)))\n",
    "            targetIndices = np.arange(TR_firstLine, TR_lastLine)\n",
    "            numTObjCk = TR_lastLine - TR_firstLine\n",
    "            redshifts = np.zeros((numTObjCk, ))\n",
    "            model_mean = np.zeros((numZ, numTObjCk, numBands))\n",
    "            model_covar = np.zeros((numZ, numTObjCk, numBands))\n",
    "            bestTypes = np.zeros((numTObjCk, ), dtype=int)\n",
    "            ells = np.zeros((numTObjCk, ), dtype=int)\n",
    "\n",
    "            # loop on training data and training GP coefficients produced by delight_learn\n",
    "            # It fills the model_mean and model_covar predicted by GP\n",
    "            loc = TR_firstLine - 1\n",
    "            trainingDataIter = getDataFromFile(params, TR_firstLine, TR_lastLine,prefix=\"training_\", ftype=\"gpparams\")\n",
    "\n",
    "            gp = PhotozGP(f_mod_interp,\n",
    "                      bandCoefAmplitudes, bandCoefPositions, bandCoefWidths,\n",
    "                      params['lines_pos'], params['lines_width'],\n",
    "                      params['V_C'], params['V_L'],\n",
    "                      params['alpha_C'], params['alpha_L'],\n",
    "                      redshiftGridGP, use_interpolators=True)\n",
    "\n",
    "            # loop on training data to load the GP parameter\n",
    "            for loc, (z, ell, bands, X, B, flatarray) in enumerate(trainingDataIter):\n",
    "                t1 = time()\n",
    "                redshifts[loc] = z              # redshift of all training samples\n",
    "                gp.setCore(X, B, nt,flatarray[0:nt+B+B*(B+1)//2])\n",
    "                bestTypes[loc] = gp.bestType   # retrieve the best-type found by delight-learn\n",
    "                ells[loc] = ell                # retrieve the luminosity parameter l\n",
    "\n",
    "                # here is the model prediction of Gaussian Process for that particular trainning galaxy\n",
    "                model_mean[:, loc, :], model_covar[:, loc, :] = gp.predictAndInterpolate(redshiftGrid, ell=ell)\n",
    "                t2 = time()\n",
    "                # print(loc, t2-t1)\n",
    "\n",
    "            #Redshift prior on training galaxy\n",
    "            # p_t = params['p_t'][bestTypes][None, :]\n",
    "            # p_z_t = params['p_z_t'][bestTypes][None, :]\n",
    "            # compute the prior for taht training sample\n",
    "            prior = np.exp(-0.5*((redshiftGrid[:, None]-redshifts[None, :]) /params['zPriorSigma'])**2)\n",
    "            # prior[prior < 1e-6] = 0\n",
    "            # prior *= p_t * redshiftGrid[:, None] *\n",
    "            # np.exp(-0.5 * redshiftGrid[:, None]**2 / p_z_t) / p_z_t\n",
    "\n",
    "            if params['useCompression'] and params['compressionFilesFound']:\n",
    "                fC = open(params['compressMargLikFile'])\n",
    "                fCI = open(params['compressIndicesFile'])\n",
    "                itCompM = itertools.islice(fC, firstLine, lastLine)\n",
    "                iterCompI = itertools.islice(fCI, firstLine, lastLine)\n",
    "\n",
    "            targetDataIter = getDataFromFile(params, firstLine, lastLine,prefix=\"target_\", getXY=False, CV=False)\n",
    "\n",
    "            if sensitivity and hyperParam_name == \"ellPriorSigma\":\n",
    "                loc, (z, normedRefFlux, bands, fluxes, fluxesVar, bCV, dCV, dVCV) = list(enumerate(targetDataIter))[0]\n",
    "                fig, axs = plt.subplots(constrained_layout=True)\n",
    "                fig2, axs2 = plt.subplots(4, 2, constrained_layout=True)\n",
    "                sigmaIter = -1\n",
    "                \n",
    "                for hyperParam in hyperParam_list:\n",
    "                    sigmaIter+=1\n",
    "                    t1 = time()\n",
    "                    ell_hat_z = normedRefFlux * 4 * np.pi * params['fluxLuminosityNorm'] * (DL(redshiftGrid)**2. * (1+redshiftGrid))\n",
    "                    ell_hat_z[:] = 1\n",
    "                    if params['useCompression'] and params['compressionFilesFound']:\n",
    "                        indices = np.array(next(iterCompI).split(' '), dtype=int)\n",
    "                        sel = np.in1d(targetIndices, indices, assume_unique=True)\n",
    "                        # same likelihood as for template fitting\n",
    "                        like_grid = approx_flux_likelihood(fluxes,fluxesVar,model_mean[:, sel, :][:, :, bands],\n",
    "                        f_mod_covar=model_covar[:, sel, :][:, :, bands],\n",
    "                        marginalizeEll=True, normalized=False,\n",
    "                        ell_hat=ell_hat_z,\n",
    "                        ell_var=(ell_hat_z*hyperParam)**2)\n",
    "                        like_grid *= prior[:, sel]\n",
    "                    else:\n",
    "                        like_grid = np.zeros((nz, model_mean.shape[1]))\n",
    "                        # same likelihood as for template fitting, but cython\n",
    "                        approx_flux_likelihood_cy(\n",
    "                            like_grid, nz, model_mean.shape[1], bands.size,\n",
    "                            fluxes, fluxesVar,  # target galaxy fluxes and variance\n",
    "                            model_mean[:, :, bands],     # prediction with Gaussian process\n",
    "                            model_covar[:, :, bands],\n",
    "                            ell_hat=ell_hat_z,           # it will find internally the ell\n",
    "                            ell_var=(ell_hat_z*hyperParam)**2)\n",
    "                        like_grid *= prior[:, :] #likelihood multiplied by redshift training galaxies priors\n",
    "                    t2 = time()\n",
    "                    localPDFs[loc, :] += like_grid.sum(axis=1)  # the final redshift posterior is sum over training galaxies posteriors\n",
    "\n",
    "                    # compute the evidence for each model\n",
    "                    evidences = np.trapz(like_grid, x=redshiftGrid, axis=0)\n",
    "                    print('Evidences shape : {}'.format(evidences.shape)\n",
    "\n",
    "                ### PLOT LIKE_GRID and/or EVIDENCES, for a SINGLE GALAXY MAYBE? ###\n",
    "                ## Plot for this iteration on ellPriorSigma:\n",
    "                plotInd = -1\n",
    "                for ligne in np.arange(4):\n",
    "                    for colonne in np.arange(2):\n",
    "                        plotInd += 1\n",
    "                        axs2[ligne, colonne].plot(redshiftGrid, like_grid[:, plotInd], label='ellPriorSigma ='+' 1e{}'.format(np.log10(ellPriorSigma)))\n",
    "                        axs2[ligne, colonne].set_xlabel('z-spec')\n",
    "                        axs2[ligne, colonne].set_ylabel('fluxLikelihood * prior')\n",
    "                        if np.all(like_grid[:, plotInd] > 0):\n",
    "                            axs2[ligne, colonne].set_yscale('log')\n",
    "                        else:\n",
    "                            axs2[ligne, colonne].set_yscale('linear')\n",
    "                        axs2[ligne, colonne].set_title('SED {}'.format(sed_names[plotInd]))\n",
    "                        axs2[ligne, colonne].legend(loc=\"upper right\")\n",
    "\n",
    "                alpha = 0.9\n",
    "                s = 5\n",
    "                axs.hist(evidences, np.arange(len(evidences)), label='ellPriorSigma ='+' 1e{}'.format(np.log10(ellPriorSigma)))\n",
    "                axs.set_xlabel('SED number')\n",
    "                axs.set_ylabel('evidences')\n",
    "                #axs.set_yscale('log')\n",
    "                axs.set_title('Evidences : likelihood integrated over redshift for each SED')\n",
    "                axs.legend(loc=\"upper right\")\n",
    "                ### WHAT ARE THE DIMENSIONS OF THE OBJECTS (8 SEDs in template fitting, how many here?) ###\n",
    "\n",
    "                t3 = time()\n",
    "\n",
    "                if params['useCompression'] and not params['compressionFilesFound']:\n",
    "                    if localCompressIndices[loc, :].sum() == 0:\n",
    "                        sortind = np.argsort(evidences)[::-1][0:Ncompress]\n",
    "                        localCompressIndices[loc, :] = targetIndices[sortind]\n",
    "                        localCompEvidences[loc, :] = evidences[sortind]\n",
    "                    else:\n",
    "                        dind = np.concatenate((targetIndices,localCompressIndices[loc, :]))\n",
    "                        devi = np.concatenate((evidences,localCompEvidences[loc, :]))\n",
    "                        sortind = np.argsort(devi)[::-1][0:Ncompress]\n",
    "                        localCompressIndices[loc, :] = dind[sortind]\n",
    "                        localCompEvidences[loc, :] = devi[sortind]\n",
    "\n",
    "                if chunk == numChunks - 1\\\n",
    "                        and redshiftsInTarget\\\n",
    "                     and localPDFs[loc, :].sum() > 0:\n",
    "                    localMetrics[loc, :] = computeMetrics(z, redshiftGrid,localPDFs[loc, :],params['confidenceLevels'])\n",
    "                t4 = time()\n",
    "                if loc % 100 == 0:\n",
    "                    print(loc, t2-t1, t3-t2, t4-t3)\n",
    "\n",
    "                if params['useCompression'] and params['compressionFilesFound']:\n",
    "                    fC.close()\n",
    "                    fCI.close()\n",
    "            else:\n",
    "                # loop on target samples\n",
    "                for loc, (z, normedRefFlux, bands, fluxes, fluxesVar, bCV, dCV, dVCV) in enumerate(targetDataIter):\n",
    "                    t1 = time()\n",
    "                    ell_hat_z = normedRefFlux * 4 * np.pi * params['fluxLuminosityNorm'] * (DL(redshiftGrid)**2. * (1+redshiftGrid))\n",
    "                    ell_hat_z[:] = 1\n",
    "                    if params['useCompression'] and params['compressionFilesFound']:\n",
    "                        indices = np.array(next(iterCompI).split(' '), dtype=int)\n",
    "                        sel = np.in1d(targetIndices, indices, assume_unique=True)\n",
    "                        # same likelihood as for template fitting\n",
    "                        like_grid2 = approx_flux_likelihood(fluxes,fluxesVar,model_mean[:, sel, :][:, :, bands],\n",
    "                        f_mod_covar=model_covar[:, sel, :][:, :, bands],\n",
    "                        marginalizeEll=True, normalized=False,\n",
    "                        ell_hat=ell_hat_z,\n",
    "                        ell_var=(ell_hat_z*params['ellPriorSigma'])**2)\n",
    "                        like_grid *= prior[:, sel]\n",
    "                    else:\n",
    "                        like_grid = np.zeros((nz, model_mean.shape[1]))\n",
    "                        # same likelihood as for template fitting, but cython\n",
    "                        approx_flux_likelihood_cy(\n",
    "                            like_grid, nz, model_mean.shape[1], bands.size,\n",
    "                            fluxes, fluxesVar,  # target galaxy fluxes and variance\n",
    "                            model_mean[:, :, bands],     # prediction with Gaussian process\n",
    "                            model_covar[:, :, bands],\n",
    "                            ell_hat=ell_hat_z,           # it will find internally the ell\n",
    "                            ell_var=(ell_hat_z*params['ellPriorSigma'])**2)\n",
    "                        like_grid *= prior[:, :] #likelihood multiplied by redshift training galaxies priors\n",
    "                    t2 = time()\n",
    "                    localPDFs[loc, :] += like_grid.sum(axis=1)  # the final redshift posterior is sum over training galaxies posteriors\n",
    "\n",
    "                    # compute the evidence for each model\n",
    "                    evidences = np.trapz(like_grid, x=redshiftGrid, axis=0)\n",
    "                    t3 = time()\n",
    "\n",
    "                    if params['useCompression'] and not params['compressionFilesFound']:\n",
    "                        if localCompressIndices[loc, :].sum() == 0:\n",
    "                            sortind = np.argsort(evidences)[::-1][0:Ncompress]\n",
    "                            localCompressIndices[loc, :] = targetIndices[sortind]\n",
    "                            localCompEvidences[loc, :] = evidences[sortind]\n",
    "                        else:\n",
    "                            dind = np.concatenate((targetIndices,localCompressIndices[loc, :]))\n",
    "                            devi = np.concatenate((evidences,localCompEvidences[loc, :]))\n",
    "                            sortind = np.argsort(devi)[::-1][0:Ncompress]\n",
    "                            localCompressIndices[loc, :] = dind[sortind]\n",
    "                            localCompEvidences[loc, :] = devi[sortind]\n",
    "\n",
    "                    if chunk == numChunks - 1\\\n",
    "                            and redshiftsInTarget\\\n",
    "                         and localPDFs[loc, :].sum() > 0:\n",
    "                        localMetrics[loc, :] = computeMetrics(z, redshiftGrid,localPDFs[loc, :],params['confidenceLevels'])\n",
    "                    t4 = time()\n",
    "                    if loc % 100 == 0:\n",
    "                        print(loc, t2-t1, t3-t2, t4-t3)\n",
    "\n",
    "                if params['useCompression'] and params['compressionFilesFound']:\n",
    "                    fC.close()\n",
    "                    fCI.close()\n",
    "\n",
    "    if threadNum == 0:\n",
    "        globalPDFs = np.zeros((numObjectsTarget, numZ))\n",
    "        globalCompressIndices = np.zeros((numObjectsTarget, Ncompress), dtype=int)\n",
    "        globalCompEvidences = np.zeros((numObjectsTarget, Ncompress))\n",
    "        globalMetrics = np.zeros((numObjectsTarget, numMetrics))\n",
    "    else:\n",
    "        globalPDFs = None\n",
    "        globalCompressIndices = None\n",
    "        globalCompEvidences = None\n",
    "        globalMetrics = None\n",
    "\n",
    "    firstLines = [int(k*numObjectsTarget/numThreads) for k in range(numThreads)]\n",
    "    lastLines = [int(min(numObjectsTarget, (k+1)*numObjectsTarget/numThreads)) for k in range(numThreads)]\n",
    "    numLines = [lastLines[k] - firstLines[k] for k in range(numThreads)]\n",
    "\n",
    "    sendcounts = tuple([numLines[k] * numZ for k in range(numThreads)])\n",
    "    displacements = tuple([firstLines[k] * numZ for k in range(numThreads)])\n",
    "    globalPDFs = localPDFs\n",
    "\n",
    "    sendcounts = tuple([numLines[k] * Ncompress for k in range(numThreads)])\n",
    "    displacements = tuple([firstLines[k] * Ncompress for k in range(numThreads)])\n",
    "    globalCompressIndices = localCompressIndices\n",
    "    globalCompEvidences = localCompEvidences\n",
    "\n",
    "    sendcounts = tuple([numLines[k] * numMetrics for k in range(numThreads)])\n",
    "    displacements = tuple([firstLines[k] * numMetrics for k in range(numThreads)])\n",
    "    globalMetrics = localMetrics\n",
    "\n",
    "    if threadNum == 0:\n",
    "        fmt = '%.2e'\n",
    "        fname = params['redshiftpdfFileComp'] if params['compressionFilesFound']\\\n",
    "            else params['redshiftpdfFile']\n",
    "        np.savetxt(fname, globalPDFs, fmt=fmt)\n",
    "        if redshiftsInTarget:\n",
    "            np.savetxt(params['metricsFile'], globalMetrics, fmt=fmt)\n",
    "        if params['useCompression'] and not params['compressionFilesFound']:\n",
    "            np.savetxt(params['compressMargLikFile'],globalCompEvidences, fmt=fmt)\n",
    "            np.savetxt(params['compressIndicesFile'],globalCompressIndices, fmt=\"%i\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
